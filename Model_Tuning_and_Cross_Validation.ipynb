{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn, sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] \n",
      "\n",
      "Matplotlib version: 2.2.2\n",
      "Numpy version: 1.14.3\n",
      "Pandas version: 0.22.0\n",
      "Sklearn version: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np),\n",
    "             ('Pandas', pd), ('Sklearn', sklearn))\n",
    "\n",
    "print(\"Python version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial to understand the trade-offs between overfitting and underfitting\n",
    "Our first step is to read in the data and prep it for modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data using a sample boston dataset from machine learning library\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Calculate Train Error & Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, model):\n",
    "    '''returns in-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_validation_error(X_test, y_test, model):\n",
    "    '''returns out-of-sample error for already fit model.'''\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n",
    "    model.fit(X_train, y_train)\n",
    "    train_error = calc_train_error(X_train, y_train, model)\n",
    "    validation_error = calc_validation_error(X_test, y_test, model)\n",
    "    return train_error, validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "\n",
    "I'm building a linear regression model of the Forest Fire dataset to investigate whether my model is underfitting, overfitting, or fitting just right. If it's under or overfitting, we'll look at one way we can correct that.\n",
    "\n",
    "Time to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 21.874 | test error: 23.817\n",
      "train/test: 1.1\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(fit_intercept=True)\n",
    "\n",
    "train_error, test_error = calc_metrics(X_train, y_train, X_test, y_test, lr)\n",
    "train_error, test_error = round(train_error, 3), round(test_error, 3)\n",
    "\n",
    "print('train error: {} | test error: {}'.format(train_error, test_error))\n",
    "print('train/test: {}'.format(round(test_error/train_error, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, my training error is considerably lower than the test error. In fact, the test error is 1.5 times worse. \n",
    "\n",
    "Which region does that put us in? That's right, it's in the high variance region, which means my model is overfitting. Again, that means my model has too much complexity. You're probably thinking, hey wait, no we're not. Drop a feature or two and then recalculate train error and test error. My response is simply: NO, DON'T EVER DO THAT. NEVER. FOR ANY REASON. \n",
    "\n",
    "Why not?\n",
    "\n",
    "Because your test set is no longer a test set. You are using it to train your model. It's the same as if you trained your model on the all the data from the outset. Seriously, don't do this. Unfortunately, practicing data scientists do this sometimes; it's one of the worst things you can do. You're almost guaranteed to produce a model that cannot generalize. \n",
    "\n",
    "So what do we do?\n",
    "\n",
    "We need to go back to the beginning. We need to split our data into three datasets: training, validation, test. Remember, the test set is data you don't touch until you're happy with your model. The test set is used ONE time to see how your model will generalize. That's it.\n",
    "\n",
    "Okay, let's take a look at this thing called a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set\n",
    "\n",
    "Three datasets from one seems like a lot of work but I promise it's worth it. \n",
    "\n",
    "First, let's see how to do this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.6% | validation: 0.2% | test 0.2%\n"
     ]
    }
   ],
   "source": [
    "# intermediate/test split (gives us test set)\n",
    "X_intermediate, X_test, y_intermediate, y_test = train_test_split(data, \n",
    "                                                                  target, \n",
    "                                                                  shuffle=True,\n",
    "                                                                  test_size=0.2, \n",
    "                                                                  random_state=15)\n",
    "\n",
    "# train/validation split (gives us train and validation sets)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n",
    "                                                                y_intermediate,\n",
    "                                                                shuffle=False,\n",
    "                                                                test_size=0.25,\n",
    "                                                                random_state=2018)\n",
    "\n",
    "# delete intermediate variables\n",
    "del X_intermediate, y_intermediate\n",
    "\n",
    "# print proportions\n",
    "print('train: {}% | validation: {}% | test {}%'.format(round(len(y_train)/len(target),2),\n",
    "                                                       round(len(y_validation)/len(target),2),\n",
    "                                                       round(len(y_test)/len(target),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit and tune our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "I want to decrease complexity. One way to do this is by using *regularization*. I won't go into the nitty gritty now because that will be for a future post. Just know that regularization is constrained optimization that imposes limits on determining model parameters. It effectively allows me to add bias to a model that's overfitting. I can control the amount of bias with a hyperparameter called *lambda* or *alpha* - you'll see both, though sklearn uses alpha because lambda is a keyword - that defines regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All errors are RMSE\n",
      "----------------------------------------------------------------------------\n",
      "alpha:   0.001 | train error: 22.93 | val error: 19.796 | test error: 23.959\n",
      "alpha:    0.01 | train error: 22.93 | val error: 19.792 | test error: 23.944\n",
      "alpha:     0.1 | train error: 22.945 | val error: 19.779 | test error: 23.818\n",
      "alpha:       1 | train error: 23.324 | val error: 20.135 | test error: 23.522\n",
      "alpha:      10 | train error: 24.214 | val error: 20.958 | test error: 23.356\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
    "print('All errors are RMSE')\n",
    "print('-'*76)\n",
    "for alpha in alphas:\n",
    "    # instantiate and fit model\n",
    "    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    # calculate errors\n",
    "    new_train_error = mean_squared_error(y_train, ridge.predict(X_train))\n",
    "    new_validation_error = mean_squared_error(y_validation, ridge.predict(X_validation))\n",
    "    new_test_error = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "    # print errors as report\n",
    "    print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {}'.\n",
    "          format(alpha,\n",
    "                 round(new_train_error,3),\n",
    "                 round(new_validation_error,3),\n",
    "                 round(new_test_error,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Data, Model, & Calculate Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15)\n",
    "\n",
    "# instantiate model\n",
    "ridge = Ridge(alpha=0.11, fit_intercept=True, random_state=99)\n",
    "\n",
    "# fit and calculate errors\n",
    "new_train_error, new_test_error = calc_metrics(X_train, y_train, X_test, y_test, ridge)\n",
    "new_train_error, new_test_error = round(new_train_error, 3), round(new_test_error, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL ERROR\n",
      "----------------------------------------\n",
      "train error: 21.874 | test error: 23.817\n",
      "\n",
      "ERROR w/REGULARIZATION\n",
      "----------------------------------------\n",
      "train error: 21.883 | test error: 23.673\n"
     ]
    }
   ],
   "source": [
    "print('ORIGINAL ERROR')\n",
    "print('-' * 40)\n",
    "print('train error: {} | test error: {}\\n'.format(train_error, test_error))\n",
    "print('ERROR w/REGULARIZATION')\n",
    "print('-' * 40)\n",
    "print('train error: {} | test error: {}'.format(new_train_error, new_test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small increase in training error coupled with a small but larger in magnitude decrease in test error. We're definitely moving in the right direction. Perhaps not quite the magnitude of change we expected, but we're simply trying to prove a point here. Remember this is a tiny dataset. Also remember I said we can do better by using something called *Cross-Validation*. Now's the time to talk about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn & CV\n",
    "\n",
    "There's two ways to do this in sklearn, pending what you want to get out of it. \n",
    "\n",
    "The first method I'll show you is `cross_val_score`, which works beautifully if all you care about is validation error.\n",
    "\n",
    "The second method is `KFold`, which is perfect if you want train and validation errors.\n",
    "\n",
    "Let's try a new model called *LASSO* just to keep things interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1]\n",
    "\n",
    "val_errors = []\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=77)\n",
    "    errors = np.sum(-cross_val_score(lasso, \n",
    "                                     data, \n",
    "                                     y=target, \n",
    "                                     scoring='neg_mean_squared_error', \n",
    "                                     cv=10, \n",
    "                                     n_jobs=-1))\n",
    "    val_errors.append(np.sqrt(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the validation errors associated with each alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.64401379981868, 18.636528438323769, 18.578057471596566, 18.503285318281634, 18.565586130742307, 21.412874355105991]\n"
     ]
    }
   ],
   "source": [
    "# RMSE\n",
    "print(val_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which value of alpha gave us the smallest validation error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "print('best alpha: {}'.format(alphas[np.argmin(val_errors)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.0001 | mean(train_error): 21.8217 | mean(val_error): 23.3633\n",
      "alpha:  0.001 | mean(train_error): 21.8221 | mean(val_error): 23.3647\n",
      "alpha:   0.01 | mean(train_error): 21.8583 | mean(val_error): 23.4126\n",
      "alpha:    0.1 | mean(train_error): 22.9727 | mean(val_error): 24.6014\n",
      "alpha:      1 | mean(train_error): 26.7371 | mean(val_error): 28.236\n",
      "alpha:   10.0 | mean(train_error):  40.183 | mean(val_error): 40.9859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "for alpha in alphas:\n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "    for train_index, val_index in kf.split(data, target):\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = target[train_index], target[val_index]\n",
    "\n",
    "        # instantiate model\n",
    "        lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=77)\n",
    "        \n",
    "        #calculate errors\n",
    "        train_error, val_error = calc_metrics(X_train, y_train, X_val, y_val, lasso)\n",
    "        \n",
    "        # append to appropriate list\n",
    "        train_errors.append(train_error)\n",
    "        validation_errors.append(val_error)\n",
    "    \n",
    "    # generate report\n",
    "    print('alpha: {:6} | mean(train_error): {:7} | mean(val_error): {}'.\n",
    "          format(alpha,\n",
    "                 round(np.mean(train_errors),4),\n",
    "                 round(np.mean(validation_errors),4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the output of *cross_val_score* to that of *KFold*, we can see that the general trend holds - an alpha of 10 results in the largest validation error. You may wonder why we get different values. The reason is that the data was split differently. We can control the splitting procedure with KFold but not cross_val_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I discussed the Bias-Variance Tradeoff where a high bias model is one that is underfit while a high variance model is one that is overfit. I splitted data into three groups for tuning purposes. Specifically, the three groups are train, validation, test. Remember the test set is used only one time to check how well a model generalizes on data it's never seen. This three-group split works exceedingly well for large datasets, not for small to medium-sized datasets, though. In that case, use cross-validation. CV can help you tune your models and extract as much signal as possible from the small data sample. Remember, with CV you don't need a test set. By using a K-fold approach, you get the equivalent of K-test sets by which to check validation error. I think helps to diagnose where you're at in the Bias-Variance regime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
